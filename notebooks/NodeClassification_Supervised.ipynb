{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from torch_geometric.datasets import Planetoid, WikipediaNetwork, Actor, WebKB\n",
    "\n",
    "from modules.model import Net\n",
    "from modules.sampling import SamplerContextMatrix, SamplerRandomWalk, SamplerFactorization, SamplerAPP\n",
    "\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     conv       dataset  train acc micro  test acc micro  train acc macro  \\\n0     GCN   0.10.10.225         0.788873        0.125628         0.788873   \n1    SAGE   0.10.10.225         0.784593        0.095477         0.784593   \n2     GCN  0.10.10.2220         0.684736        0.130653         0.684736   \n3    SAGE  0.10.10.2220         0.532097        0.080402         0.532097   \n4     GCN  0.10.10.2240         0.850214        0.170854         0.850214   \n..    ...           ...              ...             ...              ...   \n544   GAT  0.90.90.5220         0.748930        0.748744         0.748930   \n545   GAT  0.90.90.5240         0.800285        0.658291         0.800285   \n546   GAT   0.90.90.535         0.699001        0.376884         0.699001   \n547   GAT  0.90.90.5320         0.644793        0.180905         0.644793   \n548   GAT   0.90.90.545         0.613409        0.251256         0.613409   \n\n     test acc macro  label assortativity  feature assortativity  \\\n0          0.125628                  0.1                    0.1   \n1          0.095477                  0.1                    0.1   \n2          0.130653                  0.1                    0.1   \n3          0.080402                  0.1                    0.1   \n4          0.170854                  0.1                    0.1   \n..              ...                  ...                    ...   \n544        0.748744                  0.9                    0.9   \n545        0.658291                  0.9                    0.9   \n546        0.376884                  0.9                    0.9   \n547        0.180905                  0.9                    0.9   \n548        0.251256                  0.9                    0.9   \n\n     cluster coefficient  average shortest path  average degree  \n0                    0.2                      2               5  \n1                    0.2                      2               5  \n2                    0.2                      2              20  \n3                    0.2                      2              20  \n4                    0.2                      2              40  \n..                   ...                    ...             ...  \n544                  0.5                      2              20  \n545                  0.5                      2              40  \n546                  0.5                      3               5  \n547                  0.5                      3              20  \n548                  0.5                      4               5  \n\n[549 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conv</th>\n      <th>dataset</th>\n      <th>train acc micro</th>\n      <th>test acc micro</th>\n      <th>train acc macro</th>\n      <th>test acc macro</th>\n      <th>label assortativity</th>\n      <th>feature assortativity</th>\n      <th>cluster coefficient</th>\n      <th>average shortest path</th>\n      <th>average degree</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GCN</td>\n      <td>0.10.10.225</td>\n      <td>0.788873</td>\n      <td>0.125628</td>\n      <td>0.788873</td>\n      <td>0.125628</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SAGE</td>\n      <td>0.10.10.225</td>\n      <td>0.784593</td>\n      <td>0.095477</td>\n      <td>0.784593</td>\n      <td>0.095477</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GCN</td>\n      <td>0.10.10.2220</td>\n      <td>0.684736</td>\n      <td>0.130653</td>\n      <td>0.684736</td>\n      <td>0.130653</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SAGE</td>\n      <td>0.10.10.2220</td>\n      <td>0.532097</td>\n      <td>0.080402</td>\n      <td>0.532097</td>\n      <td>0.080402</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GCN</td>\n      <td>0.10.10.2240</td>\n      <td>0.850214</td>\n      <td>0.170854</td>\n      <td>0.850214</td>\n      <td>0.170854</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>2</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>544</th>\n      <td>GAT</td>\n      <td>0.90.90.5220</td>\n      <td>0.748930</td>\n      <td>0.748744</td>\n      <td>0.748930</td>\n      <td>0.748744</td>\n      <td>0.9</td>\n      <td>0.9</td>\n      <td>0.5</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>545</th>\n      <td>GAT</td>\n      <td>0.90.90.5240</td>\n      <td>0.800285</td>\n      <td>0.658291</td>\n      <td>0.800285</td>\n      <td>0.658291</td>\n      <td>0.9</td>\n      <td>0.9</td>\n      <td>0.5</td>\n      <td>2</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>546</th>\n      <td>GAT</td>\n      <td>0.90.90.535</td>\n      <td>0.699001</td>\n      <td>0.376884</td>\n      <td>0.699001</td>\n      <td>0.376884</td>\n      <td>0.9</td>\n      <td>0.9</td>\n      <td>0.5</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>547</th>\n      <td>GAT</td>\n      <td>0.90.90.5320</td>\n      <td>0.644793</td>\n      <td>0.180905</td>\n      <td>0.644793</td>\n      <td>0.180905</td>\n      <td>0.9</td>\n      <td>0.9</td>\n      <td>0.5</td>\n      <td>3</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>GAT</td>\n      <td>0.90.90.545</td>\n      <td>0.613409</td>\n      <td>0.251256</td>\n      <td>0.613409</td>\n      <td>0.251256</td>\n      <td>0.9</td>\n      <td>0.9</td>\n      <td>0.5</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>549 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = pd.read_csv('../results/supervised.csv') #pd.read_csv('../classification_on_features.csv')\n",
    "analysis = analysis.drop(columns='Unnamed: 0') \n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "synthetic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "benchmark_data_dir = \"../data_benchmark/\"\n",
    "help_data = \"../data_help/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "if synthetic:\n",
    "    datasets_names=[]\n",
    "    for l_a_trgt in [0.1,0.5,0.9]:\n",
    "                for f_a_trgt in [0.1,0.5,0.9]:\n",
    "                    for cl_trgt in [0.01,0.1,0.2,0.3,0.5]:\n",
    "                        for asp_trgt in [2,3,4,5,6,7]:\n",
    "                            for a_deg_trgt in [2,5,10,15,20,25,30,35,40]:\n",
    "                                datasets_names.append((l_a_trgt,f_a_trgt,cl_trgt,asp_trgt,a_deg_trgt))\n",
    "    def data_load(name):\n",
    "        x = torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_attr.npy'),dtype=torch.float)\n",
    "        edge_list = torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_edgelist.npy')).t()\n",
    "        y =  torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_labels.npy'))\n",
    "        data=Data(x=x,edge_index=edge_list,y=y)\n",
    "        indices=list(range(len(data.x)))\n",
    "\n",
    "        train_indices = torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "        val_indices = torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "        test_indices = torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        train_mask = torch.tensor([False]*len(indices))\n",
    "        test_mask = torch.tensor([False]*len(indices))\n",
    "        val_mask = torch.tensor([False]*len(indices))\n",
    "        train_mask[train_indices] =True\n",
    "        test_mask[test_indices]=True\n",
    "        val_mask[val_indices]=True\n",
    "        return data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask\n",
    "else:\n",
    "    datasets_names = ['Cornell','Texas','Wisconsin','Actor','Pubmed','squirrel']\n",
    "\n",
    "    def data_load(name):\n",
    "        if name == 'Cora' or name == 'Citeseer' or name == 'Pubmed':\n",
    "            data = Planetoid(root='/tmp/'+str(name), name=name,transform=T.NormalizeFeatures())[0]\n",
    "        elif name == 'Actor':\n",
    "            data = Actor(root='/tmp/actor',transform=T.NormalizeFeatures())[0]\n",
    "        elif name == \"Cornell\" or name==\"Texas\" or name==\"Wisconsin\":\n",
    "            data = WebKB(root='/tmp/'+str(name),name=name,transform=T.NormalizeFeatures())[0]\n",
    "        elif name == 'squirrel' or name=='chameleon':\n",
    "            data = WikipediaNetwork(root='/tmp/'+str(name), name=name,transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "        indices=list(range(len(data.x)))\n",
    "\n",
    "        train_indices = torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "        val_indices = torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "        test_indices = torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        train_mask = torch.tensor([False]*len(indices))\n",
    "        test_mask = torch.tensor([False]*len(indices))\n",
    "        val_mask = torch.tensor([False]*len(indices))\n",
    "        train_mask[train_indices] =True\n",
    "        test_mask[test_indices]=True\n",
    "        val_mask[val_indices]=True\n",
    "        return data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import collections\n",
    "\n",
    "class Main:\n",
    "    def __init__(self,name, conv, device, loss_function, mode):\n",
    "        data, train_indices, val_indices, test_indices, train_mask, val_mask, test_mask = data_load(name)\n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.x = data.x\n",
    "        self.y = data.y.squeeze()\n",
    "        self.data=data.to(device)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.datasetname=name\n",
    "        self.train_indices =train_indices# torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "        self.val_indices =val_indices# torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "        self.test_indices = test_indices#torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        self.train_mask = train_mask#torch.tensor([False]*len(indices))\n",
    "        self.test_mask = test_mask#torch.tensor([False]*len(indices))\n",
    "        self.val_mask =val_mask# torch.tensor([False]*len(indices))\n",
    "        super(Main, self).__init__()\n",
    "    def train(self, model,data,optimizer,train_loader,dropout,epoch):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "       # print('train loader',len(train_loader))\n",
    "        \n",
    "        if model.mode == 'unsupervised':\n",
    "            pass\n",
    "        elif model.mode== 'supervised':\n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(self.device),dp=dropout)\n",
    "                y = self.y.type(torch.LongTensor)\n",
    "                y = y.to(self.device)\n",
    "                loss = model.loss_sup(out[self.train_mask],y[self.train_mask])\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id].to(self.device), adjs)\n",
    "                    y = self.y.type(torch.LongTensor)\n",
    "                    y = y.to(self.device)\n",
    "                    loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                    total_loss += loss\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model, data):\n",
    "        model.eval()\n",
    "        out = model.inference(data.to(self.device))\n",
    "\n",
    "        y_true = self.y.cpu().detach().numpy()\n",
    "        self.y=self.y.cpu()\n",
    "        if model.mode == 'supervised':\n",
    "            y_true = self.y.unsqueeze(-1)\n",
    "            y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "\n",
    "            accs_micro = []\n",
    "            accs_macro = []\n",
    "            for mask in [self.train_mask,self.test_mask,self.val_mask]:\n",
    "                accs_micro += [accuracy_score(self.y.detach()[mask].cpu().numpy(),y_pred[mask])]\n",
    "                accs_macro += [accuracy_score(self.y.detach()[mask].cpu().numpy(),y_pred[mask])]\n",
    "\n",
    "            return out,accs_micro,accs_macro\n",
    "\n",
    "        elif model.mode == 'unsupervised':\n",
    "              pass\n",
    "\n",
    "    def run(self,params):\n",
    "\n",
    "        hidden_layer=params['hidden_layer']\n",
    "        out_layer=params['out_layer']\n",
    "        dropout=params['dropout']\n",
    "        size=params['size of network, number of convs']\n",
    "        learning_rate=params['lr']\n",
    "        hidden_layer_for_classifier = params['hidden_layer_for_classifier']\n",
    "        number_of_layers_for_classifier = params['number_of_layers_for_classifier']\n",
    "        heads = params['heads']\n",
    "\n",
    "        #hidden_layer=64,out_layer=128,dropout=0.0,size=1,learning_rate=0.001,c=100\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=self.train_mask, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout,number_of_layers_for_classifier=number_of_layers_for_classifier, hidden_layer_for_classifier=hidden_layer_for_classifier,heads=heads)\n",
    "        model.to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs_mi=[]\n",
    "        test_accs_mi=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv\n",
    "        train_accs_ma = []\n",
    "        test_accs_ma = []\n",
    "        print(name_of_plot)\n",
    "        log = 'Loss: {:.4f}, Epoch: {:03d}, Train acc micro: {:.4f}, Test acc micro: {:.4f},Train acc macro: {:.4f}, Test acc macro: {:.4f}'\n",
    "\n",
    "        for epoch in range(100):\n",
    "                    print(epoch)\n",
    "                    loss = self.train(model,self.data,optimizer,train_loader,dropout,epoch)\n",
    "                    losses.append(loss.detach().cpu())\n",
    "                    out, [train_acc_mi, test_acc_mi,val_acc_mi],[train_acc_ma, test_acc_ma,val_acc_ma] = self.test(model,self.data)\n",
    "                    train_accs_mi.append(train_acc_mi)\n",
    "                    test_accs_mi.append(test_acc_mi)\n",
    "                    train_accs_ma.append(train_acc_ma)\n",
    "                    test_accs_ma.append(test_acc_ma)\n",
    "                    print(log.format(loss, epoch, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma))\n",
    "        #np.save('../data_help/embedings_'+str(self.datasetname)+str(self.loss['name'])+'.npy', out.cpu().numpy())\n",
    "                    \n",
    "                     #scheduler.step()\n",
    "        print(epoch, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs_mi)\n",
    "        plt.title(name_of_plot+' test f1 micro')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "                  \n",
    "        plt.plot(test_accs_ma)\n",
    "        plt.title(name_of_plot+' test f1 macro')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        return train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-3,1e-2)\n",
    "        heads = trial.suggest_categorical('heads',[2])\n",
    "        #c =trial.suggest_categorical(\"c\",  [0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,20,30,100])\n",
    "        hidden_layer_for_classifier = trial.suggest_categorical(\"hidden_layer_for_classifier\", [32,64,128,256])\n",
    "        number_of_layers_for_classifier = trial.suggest_categorical(\"number_of_layers_for_classifier\", [1,2,3])\n",
    "        # варьируем параметры\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=Conv,loss_function={'name':'supervised'},device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout,heads = heads)\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "        print('after train loader', len(self.data.x), sum(self.train_mask), len(collections.Counter(self.data.edge_index[0].tolist())))\n",
    "\n",
    "        model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "\n",
    "        for epoch in range(50):\n",
    "            loss = self.train(model,self.data,optimizer,train_loader,dropout,epoch)\n",
    "        _, [train_acc_mi, test_acc_mi,val_acc_mi], [train_acc_ma, test_acc_ma,val_acc_ma] = self.test(model=model, data=self.data)\n",
    "        trial.report(np.sqrt(val_acc_mi*val_acc_ma), epoch)\n",
    "        return np.sqrt(val_acc_mi*val_acc_ma)\n",
    "\n",
    "    def run(self,number_of_trials):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "        trial = study.best_trial\n",
    "        return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "analysis = pd.DataFrame(columns=analysis.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-07 16:58:53,025]\u001B[0m A new study created in memory with name: no-name-b01188af-19ad-4840-988e-82cae3878f9d\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train loader 1000 tensor(701) 989\n",
      "(701, 701)\n",
      "(701, 701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m[W 2022-12-07 16:58:53,431]\u001B[0m Trial 0 failed because of the following error: RuntimeError('mat1 and mat2 shapes cannot be multiplied (701x128 and 256x128)')\u001B[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3008\\3462569007.py\", line 23, in objective\n",
      "    loss = self.train(model,self.data,optimizer,train_loader,dropout,epoch)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3008\\1264338756.py\", line 43, in train\n",
      "    out = model.forward(data.x[n_id].to(self.device), adjs)\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\modules\\model.py\", line 115, in forward\n",
      "    for j in range(self.number_of_layers_for_classifier):\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (701x128 and 256x128)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (701x128 and 256x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3008\\966001044.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'conv'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mconv\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m&\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dataset'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m                 \u001B[0mMO\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMainOptuna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'HOPE_RPR'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;34m'supervised'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m                 \u001B[0mbest_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMO\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnumber_of_trials\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m500\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m                 \u001B[0mM\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;34m'HOPE_RPR'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;34m'supervised'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3008\\3462569007.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, number_of_trials)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[0mstudy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptuna\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreate_study\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirection\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"maximize\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m         \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobjective\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_trials\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnumber_of_trials\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     32\u001B[0m         \u001B[0mtrial\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_trial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtrial\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\study.py\u001B[0m in \u001B[0;36moptimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    426\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    427\u001B[0m             \u001B[0mgc_after_trial\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgc_after_trial\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 428\u001B[1;33m             \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    429\u001B[0m         )\n\u001B[0;32m    430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     74\u001B[0m                 \u001B[0mreseed_sampler_rng\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m                 \u001B[0mtime_start\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m                 \u001B[0mprogress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprogress_bar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m             )\n\u001B[0;32m     78\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 160\u001B[1;33m             \u001B[0mfrozen_trial\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_run_trial\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstudy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    161\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m             \u001B[1;31m# The following line mitigates memory problems that can be occurred in some\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    232\u001B[0m         \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc_err\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    233\u001B[0m     ):\n\u001B[1;32m--> 234\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mfunc_err\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    235\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mfrozen_trial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    194\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mget_heartbeat_thread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_trial_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_storage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    195\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 196\u001B[1;33m             \u001B[0mvalue_or_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    197\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mexceptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrialPruned\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m             \u001B[1;31m# TODO(mamu): Handle multi-objective cases.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3008\\3462569007.py\u001B[0m in \u001B[0;36mobjective\u001B[1;34m(self, trial)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m50\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m         \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtrain_acc_mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_acc_mi\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_acc_mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtrain_acc_ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_acc_ma\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_acc_ma\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mtrial\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreport\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_acc_mi\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mval_acc_ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3008\\1264338756.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, model, data, optimizer, train_loader, dropout, epoch)\u001B[0m\n\u001B[0;32m     41\u001B[0m                         \u001B[0madjs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0madjs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m                     \u001B[0madjs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0madj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0madj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0madjs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m                     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mn_id\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madjs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m                     \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m                     \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\modules\\model.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x, adjs)\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    114\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"supervised\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 115\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumber_of_layers_for_classifier\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    116\u001B[0m                 \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    117\u001B[0m                 \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 114\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (701x128 and 256x128)"
     ]
    }
   ],
   "source": [
    "#analysis=pd.read_csv('../results/supervised.csv')\n",
    "#analysis=analysis.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "        for conv in ['GCN']:\n",
    "            if len(analysis[(analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function='HOPE_RPR',mode= 'supervised')\n",
    "                best_values = MO.run(number_of_trials=500)\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function={'name':'HOPE_RPR'},mode= 'supervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,l,f,cl,asp,ad],index = analysis.columns)\n",
    "                #analysis = analysis.append(to_append,ignore_index=True)\n",
    "               # analysis.to_csv('../results/supervised.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(analysis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis = pd.read_csv('../results/final_data.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "analysis = analysis[(analysis['loss'] == 'features') & (analysis['label assortativity']==0.1) & (analysis['cluster coefficient']==0.2) & (analysis['feature assortativity']==0.1) & (analysis['average shortest path']==3) & (analysis['average degree']==5)]\n",
    "analysis['test acc micro']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "conv = 'GCN'\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_names=[(0.9, 0.5, 0.01, 2, 5)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LapEigen\n",
    "loss_name = 'LapEigen'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "conv = 'GCN'\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis=analysis.drop(columns=['Unnamed: 0'])\n",
    "conv = 'GCN'\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = Node2Vec\n",
    "loss_name = 'Node2Vec'\n",
    "device= 'cpu'\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "for name in ['Cornell']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                loss_trgt[\"p\"] = best_values['p']\n",
    "                loss_trgt[\"q\"] = best_values['q']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_PPR\n",
    "loss_name = 'VERSE_PPR'\n",
    "\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#on real graphs\n",
    "loss = Force2Vec\n",
    "loss_name = 'Force2Vec'\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "for name in ['Cornell']:\n",
    "    for conv in ['GCN']:\n",
    "         if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                MO = MainOptuna(name = name, conv = conv, device = device, loss_function = loss, mode = 'unsupervised')\n",
    "                best_values = MO.run(number_of_trials =500)\n",
    "\n",
    "                loss_trgt = dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name, conv=conv, device=device, loss_function=loss_trgt, mode='unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_SR\n",
    "loss_name = 'VERSE_SR'\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "for name in ['Cornell']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis = analysis.drop(columns=['Unnamed: 0'])\n",
    "for name in ['Cornell']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modkdjfjf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'\n",
    "number_of_trials = 100\n",
    "import os\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "        if len(analysis[(analysis['la'] == l)&(analysis['fa']==f)&(analysis['cl']==cl)&(analysis['asp']==asp)&(analysis['ad']==ad)] ) == 0:\n",
    "            data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask = data_load(name)\n",
    "            x = data.x.detach()\n",
    "            y = data.y.detach()\n",
    "            def objective(trial):\n",
    "            # Integer parameter\n",
    "                c = trial.suggest_categorical(\"c\",  [0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,20,30,100])\n",
    "                clf = LogisticRegression(max_iter = 3000, C=c).fit(x[train_mask].numpy(), y[train_mask].numpy())\n",
    "\n",
    "                accs_micro = []\n",
    "                accs_macro = []\n",
    "                for mask in [train_mask,test_mask,val_mask]:\n",
    "                    accs_micro += [accuracy_score(data.y.detach()[mask].numpy(),clf.predict(x[mask].numpy()))]\n",
    "                    accs_macro += [accuracy_score(data.y.detach()[mask].numpy(),clf.predict(x[mask].numpy()))]\n",
    "\n",
    "                return np.sqrt(accs_micro[2]*accs_macro[2])\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials = number_of_trials)\n",
    "            trial = study.best_trial\n",
    "            c=trial.params['c']\n",
    "            clf = LogisticRegression(max_iter = 3000, C=c).fit(x[train_mask].numpy(), y[train_mask].numpy())\n",
    "            accs_micro = []\n",
    "            accs_macro = []\n",
    "            for mask in [train_mask,test_mask,val_mask]:\n",
    "                accs_micro += [f1_score(y[mask].numpy(),clf.predict(x[mask].numpy()), average='micro')]\n",
    "                accs_macro += [f1_score(y[mask].numpy(),clf.predict(x[mask].numpy()), average='macro')]\n",
    "\n",
    "            to_append = pd.Series([l,f,cl,asp,ad, accs_micro[0],accs_micro[1], accs_macro[0] , accs_macro[1]],index = analysis.columns)\n",
    "            analysis = analysis.append(to_append, ignore_index=True)\n",
    "            analysis.to_csv('classification_on_features.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = Force2Vec\n",
    "loss_name = 'Force2Vec'\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "        print('hey')\n",
    "        for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name = name, conv = conv, device = device, loss_function = loss, mode = 'unsupervised')\n",
    "                best_values = MO.run(number_of_trials = 500)\n",
    "\n",
    "                loss_trgt = dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_force2vec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_SR\n",
    "loss_name = 'VERSE_SR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_PPR\n",
    "loss_name = 'VERSE_PPR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = LapEigen\n",
    "loss_name = 'LapEigen'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = GraphFactorization\n",
    "loss_name = 'GraphFactorization'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi, train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_AA\n",
    "loss_name = 'HOPE_AA'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_RPR\n",
    "loss_name = 'HOPE_RPR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_Katz\n",
    "loss_name = 'HOPE_Katz'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"betta\"] = best_values['betta']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = Node2Vec\n",
    "loss_name = 'Node2Vec'\n",
    "device = 'cpu'\n",
    "for name in ['chameleon']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                loss_trgt[\"p\"] = best_values['p']\n",
    "                loss_trgt[\"q\"] = best_values['q']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = DeepWalk\n",
    "loss_name = 'DeepWalk'\n",
    "device='cpu'\n",
    "for name in ['Citeseer']:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = APP\n",
    "loss_name = 'APP'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}