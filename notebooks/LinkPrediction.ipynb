{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch_geometric.datasets import Planetoid, WikipediaNetwork, Actor, WebKB\n",
    "\n",
    "from modules.model import Net\n",
    "from modules.sampling import SamplerContextMatrix, SamplerRandomWalk, SamplerFactorization, SamplerAPP\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synthetic = True\n",
    "benchmark_data_dir = \"../data_benchmark/\"\n",
    "help_data = \"../data_help/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if synthetic:\n",
    "    datasets_names=[]\n",
    "    for l_a_trgt in [0.1,0.5,0.9]:\n",
    "                for f_a_trgt in [0.1,0.5,0.9]:\n",
    "                    for cl_trgt in [0.01,0.1,0.2,0.3,0.5]:\n",
    "                        for asp_trgt in [2,3,4,5,6,7]:\n",
    "                            for a_deg_trgt in [2,5,10,15,20,25,30,35,40]:\n",
    "                                datasets_names.append((l_a_trgt,f_a_trgt,cl_trgt,asp_trgt,a_deg_trgt))\n",
    "    def data_load(name):\n",
    "        x = torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_attr.npy'),dtype=torch.float)\n",
    "        edge_list = torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_edgelist.npy')).t()\n",
    "\n",
    "        data=Data(x=x,edge_index=edge_list)\n",
    "        indices=list(range(len(data.x)))\n",
    "\n",
    "        train_indices = random.sample(list(range(len(indices))), k = int(len(indices)*0.7))\n",
    "        train_indices_torch = torch.tensor(train_indices)\n",
    "\n",
    "        test_indices = list((set(indices)-set(train_indices)))\n",
    "        test_indices_torch = torch.tensor(test_indices)\n",
    "        train_mask = torch.tensor([False]*len(indices))\n",
    "        test_mask = torch.tensor([False]*len(indices))\n",
    "        train_mask[train_indices] = True\n",
    "        test_mask[test_indices]= True\n",
    "\n",
    "        return data, train_indices_torch,test_indices_torch,train_mask,test_mask\n",
    "else:\n",
    "    datasets_names = ['Cornell','Texas','Wisconsin','Actor','Pubmed','squirrel']\n",
    "\n",
    "    def data_load(name):\n",
    "        if name == 'Cora' or name == 'Citeseer' or name == 'Pubmed':\n",
    "            data = Planetoid(root='/tmp/'+str(name), name=name,transform=T.NormalizeFeatures())[0]\n",
    "        elif name == 'Actor':\n",
    "            data = Actor(root='/tmp/actor',transform=T.NormalizeFeatures())[0]\n",
    "        elif name == \"Cornell\" or name==\"Texas\" or name==\"Wisconsin\":\n",
    "            data = WebKB(root='/tmp/'+str(name),name=name,transform=T.NormalizeFeatures())[0]\n",
    "        elif name == 'squirrel' or name=='chameleon':\n",
    "            data = WikipediaNetwork(root='/tmp/'+str(name), name=name,transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "        indices=list(range(len(data.x)))\n",
    "\n",
    "        train_indices = torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "        val_indices = torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "        test_indices = torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        train_mask = torch.tensor([False]*len(indices))\n",
    "        test_mask = torch.tensor([False]*len(indices))\n",
    "        val_mask = torch.tensor([False]*len(indices))\n",
    "        train_mask[train_indices] =True\n",
    "        test_mask[test_indices]=True\n",
    "        val_mask[val_indices]=True\n",
    "        return data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synthetic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#loss functions\n",
    "\n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\":SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk_length\":[5, 10, 15, 20],\"walks_per_node\":[5, 10, 15, 20],\"num_negative_samples\":[1,6, 11, 16, 21],\"context_size\" : [5, 10, 15, 20],\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\" : SamplerRandomWalk } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk_length\":[5, 10, 15, 20],\"walks_per_node\":[5, 10, 15, 20],\"num_negative_samples\":[1,6, 11, 16, 21],\"context_size\" : [5, 10, 15, 20],\"p\": [0.25, 0.50, 1, 2, 4] ,\"q\":[0.25, 0.50, 1, 2, 4], \"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\": SamplerRandomWalk}#то же самое\n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerAPP}\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"betta\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "\n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\", \"C\":\"CN\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "\n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "\n",
    "Force2Vec = {\"Name\": \"Force2Vec\",\"C\": \"Adj\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Force2Vec\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from modules.negativeSampling import NegativeSampler\n",
    "\n",
    "class Main:\n",
    "    def __init__(self,name, conv, device, loss_function, mode):\n",
    "        data, train_indices,test_indices,train_mask,test_mask = data_load(name)\n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.x = data.x\n",
    "        self.data=data.to(device)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.datasetname=name\n",
    "        self.train_indices =train_indices# torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "\n",
    "        self.test_indices = test_indices#torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        self.train_mask = train_mask#torch.tensor([False]*len(indices))\n",
    "        self.test_mask = test_mask#torch.tensor([False]*len(indices))\n",
    "\n",
    "        self.flag = self.loss[\"flag_tosave\"]\n",
    "        super(Main, self).__init__()\n",
    "    def sampling(self,Sampler, epoch, nodes, loss,train_flag = True):\n",
    "            if (epoch == 0):\n",
    "                if self.flag:\n",
    "                    if \"alpha\" in self.loss:\n",
    "                        name_of_file = self.datasetname+\"_samples_\"+loss[\"Name\"]+\"_alpha_\"+str(loss[\"alpha\"])+\".pickle\"\n",
    "                    elif \"betta\" in self.loss:\n",
    "                        name_of_file = self.datasetname+\"_samples_\"+loss[\"Name\"]+\"_betta_\"+str(loss[\"betta\"])+\".pickle\"\n",
    "                    else:\n",
    "                        name_of_file = self.datasetname+\"_samples_\"+loss[\"Name\"]+\".pickle\"\n",
    "\n",
    "                    if os.path.exists(f'{help_data}/'+str(name_of_file)):\n",
    "                        with open(f'{help_data}/'+str(name_of_file),'rb') as f:\n",
    "                            self.samples = pickle.load(f)\n",
    "                    else:\n",
    "                        self.samples = Sampler.sample(nodes)\n",
    "                        with open(f'{help_data}/'+str(name_of_file),'wb') as f:\n",
    "                            pickle.dump(self.samples,f)\n",
    "                else:\n",
    "                    self.samples = Sampler.sample(nodes,train_flag)\n",
    " \n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch,loss):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "       # print('train loader',len(train_loader))\n",
    "        \n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr = torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                #print('before',data.x)\n",
    "                out = model.inference(data.to(self.device),dp=dropout)\n",
    "                #print('after',out, sum(sum(out)))\n",
    "\n",
    "                samples = self.sampling(Sampler,epoch, indices_of_train_data,loss,train_flag=True)\n",
    "                loss = model.loss(out[self.train_mask], self.samples)\n",
    "                #print('loss',loss)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(self.device)].to(self.device), adjs)\n",
    "\n",
    "                    self.sampling(Sampler,epoch,n_id[:batch_size],loss,train_flag=True)\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader), out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model, data,test_loader,epoch,Sampler,loss):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr = torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                #print('before',data.x)\n",
    "                out = model.inference(data.to(self.device),dp=0)\n",
    "                #print('after',out, sum(sum(out)))\n",
    "                samples = self.sampling(Sampler,epoch, indices_of_train_data,loss,train_flag=False)\n",
    "                loss = model.loss(out[self.test_mask], self.samples)\n",
    "                #print('loss',loss)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in test_loader:\n",
    "                    if len(test_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(self.device)].to(self.device), adjs)\n",
    "\n",
    "                    self.sampling(Sampler,epoch,n_id[:batch_size],loss,train_flag=False)\n",
    "\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "\n",
    "            return total_loss /len(test_loader)\n",
    "\n",
    "    def run(self,params):\n",
    "        hidden_layer=params['hidden_layer']\n",
    "        out_layer=params['out_layer']\n",
    "        dropout=params['dropout']\n",
    "        size=params['size of network, number of convs']\n",
    "        learning_rate=params['lr']\n",
    "        #hidden_layer_for_classifier=params['hidden_layer_for_classifier']\n",
    "        #alpha_for_classifier = params['alpha_for_classifier']\n",
    "        #learning_rate_for_classifier = params['learning_rate_for_classifier']\n",
    "        #n_layers_for_classifier = params['n_layers_for_classifier']\n",
    "\n",
    "        #hidden_layer=64,out_layer=128,dropout=0.0,size=1,learning_rate=0.001,c=100\n",
    "        classifier = \"logistic regression\"\n",
    "        self.data.edge_index= self.data.edge_index.type(torch.LongTensor)\n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=torch.BoolTensor([True]*len(self.data.x)), batch_size = int(len(self.data.x)), sizes=[-1]*size)\n",
    "        \n",
    "        Sampler = self.loss[\"Sampler\"]\n",
    "        LossSampler = Sampler(self.datasetname, self.data, device=device, mask=torch.BoolTensor([True]*len(self.data.x)), loss_info=self.loss, help_dir=help_data)\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        for epoch in range(100):\n",
    "                    loss,out = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch,self.loss)\n",
    "\n",
    "        np.save(str(self.datasetname)+'_'+ str(self.loss['Name'])+'_emb.npy', out.detach().cpu().numpy())\n",
    "\n",
    "        positive_edges = []\n",
    "        for edge in self.data.edge_index.T.tolist():\n",
    "            if edge[0] in self.train_indices:\n",
    "                if edge[1] in self.train_indices:\n",
    "                    positive_edges.append(edge)\n",
    "\n",
    "        ns = NegativeSampler(data = self.data)\n",
    "        num_negative_samples = int(len(positive_edges)/len(self.train_indices))\n",
    "        neg_samples = ns.negative_sampling( self.train_indices, num_negative_samples=num_negative_samples)\n",
    "\n",
    "        # find treshold\n",
    "        max_acc = 0\n",
    "        max_acc_k = 0\n",
    "\n",
    "        true = [1]*len(positive_edges)+[0]*len(neg_samples)\n",
    "        emb_norm = torch.nn.functional.normalize(torch.tensor(out.detach().cpu()))\n",
    "        for k in np.linspace(0,1,100).tolist():\n",
    "            pred = []\n",
    "            for edge in positive_edges:\n",
    "                pred.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])).tolist() > k)\n",
    "            for edge in neg_samples:\n",
    "                pred.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])).tolist() > k)\n",
    "            acc = accuracy_score(true,pred)\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                max_acc_k = k\n",
    "\n",
    "        positive_edges_test = []\n",
    "        for edge in self.data.edge_index.T.tolist():\n",
    "            if edge[0] in self.test_indices:\n",
    "                if edge[1] in self.test_indices:\n",
    "                    positive_edges_test.append(edge)\n",
    "\n",
    "        num_neg_samples_test = int(len(positive_edges)/len(self.test_indices))\n",
    "        ns = NegativeSampler(data=self.data)\n",
    "        neg_samples_test = ns.negative_sampling( self.test_indices, num_negative_samples=num_neg_samples_test)\n",
    "        pred_test=[]\n",
    "        for edge in positive_edges_test:\n",
    "            pred_test.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])) > max_acc_k)\n",
    "        #print(torch.sigmoid(torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])))\n",
    "        for edge in neg_samples_test:\n",
    "            pred_test.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])) > max_acc_k)\n",
    "\n",
    "        true_test = [1]*len(positive_edges_test)+[0]*len(neg_samples_test)\n",
    "\n",
    "        return accuracy_score(true_test, pred_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-3,1e-2)\n",
    "     #   learning_rate_for_classifier =trial.suggest_float(\"learning_rate_for_classifier\",5e-3,1e-2)\n",
    "      #  n_layers_for_classifier = trial.suggest_categorical(\"n_layers_for_classifier\", [1,2,3])\n",
    "       # alpha_for_classifier = trial.suggest_categorical(\"alpha_for_classifier\",  [0.001, 0.01, 0.1,0.3,0.5,0.7,0.9,1,10,20,30,100])\n",
    "        #c =trial.suggest_categorical(\"c\",  [0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,20,30,100])\n",
    "        #hidden_layer_for_classifier = trial.suggest_categorical(\"hidden_layer_for_classifier\", [32,64,128,256])\n",
    "        # варьируем параметры\n",
    "        loss_to_train={}\n",
    "        for name in self.loss:\n",
    "            \n",
    "            if type(self.loss[name]) == list :\n",
    "                if len(self.loss[name]) == 3:\n",
    "                    var = trial.suggest_int(name,self.loss[name][0],self.loss[name][1],step=self.loss[name][2])\n",
    "                    loss_to_train[name] = var\n",
    "                elif len(self.loss[name]) == 2:\n",
    "                    var_2 = trial.suggest_float(name,self.loss[name][0],self.loss[name][1])\n",
    "                    loss_to_train[name] = var_2\n",
    "                else:\n",
    "                    var_3 = trial.suggest_categorical(name, self.loss[name])\n",
    "                    loss_to_train[name] = var_3\n",
    "            else:\n",
    "                loss_to_train[name] = self.loss[name]\n",
    "        if name =='q' and type(self.loss[name]) == list:\n",
    "            var_5 = trial.suggest_categorical('p', self.loss['p'])\n",
    "            var_4 = trial.suggest_categorical('q', self.loss[name])\n",
    "            if var_4 > 1:\n",
    "                var_4=1\n",
    "            if var_5 < var_4:     \n",
    "                var_5=var_4\n",
    "            loss_to_train['q'] = var_4\n",
    "            loss_to_train['p'] = var_5\n",
    "                \n",
    "        Sampler =loss_to_train[\"Sampler\"]\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=Conv,loss_function=loss_to_train,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "        self.data.edge_index= self.data.edge_index.type(torch.LongTensor)\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "        test_loader = NeighborSampler(self.data.edge_index,  batch_size = int(sum(self.test_mask)),node_idx=self.test_mask, sizes=[-1]*size)\n",
    "        LossSampler = Sampler(self.datasetname,self.data,device=self.device,mask=self.train_mask,loss_info=loss_to_train, help_dir=help_data)\n",
    "        LossSamplerTest = Sampler(self.datasetname,self.data,device=self.device,mask=self.test_mask,loss_info=loss_to_train, help_dir=help_data)\n",
    "        model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        for epoch in range(50):\n",
    "            loss,_ = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch,loss_to_train)\n",
    "        loss_test = self.test(model=model, data=self.data, epoch=0, test_loader=test_loader,Sampler=LossSamplerTest,loss=loss_to_train)\n",
    "        trial.report(loss_test, epoch)\n",
    "        return loss_test\n",
    "\n",
    "    def run(self,number_of_trials):\n",
    "        study = optuna.create_study(direction=\"minimize\",study_name=self.loss[\"Name\"]+\" loss,\"+str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "        trial = study.best_trial\n",
    "        return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "#results_df=pd.DataFrame(columns=['loss','conv','fa', 'cl', 'asp','ad','dataset','acc lp','best_values'])\n",
    "results_df = pd.read_csv('results_on_LP_random_split.csv')\n",
    "results_df = results_df.drop(columns=['Unnamed: 0'])\n",
    "device =torch.device('cuda')\n",
    "\n",
    "for conv in ['SAGE','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names[950:1620]:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(results_df[ (results_df['loss'] == loss_name) & (results_df['conv'] == conv) & (results_df['dataset'] == name)] ) == 0:\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function = loss,mode = 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "                    loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    accuracy = M.run(best_values)\n",
    "\n",
    "                    to_append=pd.Series([loss_name, conv, f,cl,asp,ad,name , accuracy, best_values],index = results_df.columns)\n",
    "                    results_df = results_df.append(to_append,ignore_index=True)\n",
    "                    results_df.to_csv('results_on_LP_random_split.csv')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def LINE_func_train(attrs):\n",
    "    (l,f,cl,asp,ad,conv) = attrs\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if len(results_df[(results_df['loss'] == loss_name) & (results_df['conv'] == conv) & (results_df['dataset'] == name)] ) == 0:\n",
    "            MO = MainOptuna(name=name,conv=conv, device=device, loss_function = loss,mode = 'unsupervised')\n",
    "            best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "            loss_trgt=dict()\n",
    "            for par in loss:\n",
    "                loss_trgt[par]=loss[par]\n",
    "\n",
    "            loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "            loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "            M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "            accuracy = M.run(best_values)\n",
    "\n",
    "            to_append=pd.Series([loss_name, conv, f,cl,asp,ad,name , accuracy, best_values],index = results_df.columns)\n",
    "            return to_append"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attrs = []\n",
    "for conv in ['SAGE','GAT']:\n",
    "    for (a,b,c,d,e) in datasets_names[810:1620]:\n",
    "        attrs.append((a,b,c,d,e,conv))\n",
    "len(attrs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# cuda без параллеливания 500 триалов 1 датасет за 7 минут, 9 датасетов чуть больше часа"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "\n",
    "results_df = pd.read_csv('results_on_LP_random_split.csv')\n",
    "results_df = results_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "device = torch.device('cpu') #('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pool = mp.Pool(4)\n",
    "print('start')\n",
    "a = datetime.now()\n",
    "to_append = pool.map(LINE_func_train, attrs[12:14])\n",
    "print(to_append)\n",
    "            #analysis = analysis.append(to_append,ignore_index=True)\n",
    "            #analysis.to_csv('../results/classification_with_MLP.csv')\n",
    "pool.close()\n",
    "b = datetime.close()\n",
    "print('time',b-a)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_values={'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 3, 'lr': 0.008061289273891234, 'num_negative_samples': 1, 'lmbda': 0.48186987089486877}\n",
    "loss_trgt=LINE\n",
    "loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "M.run(best_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "analysis = pd.read_csv('../results/classification_with_MLP.csv')\n",
    "analysis = analysis.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "for conv in ['SAGE','GCN','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "                    loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                    to_append=pd.Series([loss_name, conv, l,f,cl,asp,ad,name,train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,best_values],index = analysis.columns)\n",
    "                    analysis = analysis.append(to_append,ignore_index=True)\n",
    "                    #analysis.to_csv('../results/classification_with_MLP.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "analysis = pd.read_csv('../results/classification_with_MLP.csv')\n",
    "analysis = analysis.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "for conv in ['SAGE','GCN','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                    to_append=pd.Series([loss_name, conv, l,f,cl,asp,ad,name,train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,best_values],index = analysis.columns)\n",
    "                    analysis = analysis.append(to_append,ignore_index=True)\n",
    "                    analysis.to_csv('../results/classification_with_MLP.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_with_MLP.csv')\n",
    "analysis = analysis.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "for conv in ['SAGE','GCN','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "                    loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "                    to_append=pd.Series([loss_name, conv, l,f,cl,asp,ad,name,train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,best_values],index = analysis.columns)\n",
    "                    analysis = analysis.append(to_append,ignore_index=True)\n",
    "                    analysis.to_csv('../results/classification_with_MLP.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ниже надо еще редактировать"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LapEigen\n",
    "loss_name = 'LapEigen'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis=analysis.drop(columns=['Unnamed: 0'])\n",
    "for name in datasets_names[3:4]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = Node2Vec\n",
    "loss_name = 'Node2Vec'\n",
    "device= 'cpu'\n",
    "for name in ['Cornell']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                loss_trgt[\"p\"] = best_values['p']\n",
    "                loss_trgt[\"q\"] = best_values['q']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_PPR\n",
    "loss_name = 'VERSE_PPR'\n",
    "\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#on real graphs\n",
    "loss = Force2Vec\n",
    "loss_name = 'Force2Vec'\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "for name in datasets_names[2:]:\n",
    "    for conv in ['GCN']:\n",
    "         if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                MO = MainOptuna(name = name, conv = conv, device = device, loss_function = loss, mode = 'unsupervised')\n",
    "                best_values = MO.run(number_of_trials =500)\n",
    "\n",
    "                loss_trgt = dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name, conv=conv, device=device, loss_function=loss_trgt, mode='unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VERSE_SR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6872\\1528736947.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mVERSE_SR\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mloss_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'VERSE_SR'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdatasets_names\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mconv\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'GCN'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'VERSE_SR' is not defined"
     ]
    }
   ],
   "source": [
    "loss = VERSE_SR\n",
    "loss_name = 'VERSE_SR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modkdjfjf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for (l,f,cl,asp,ad) in datasets_names:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'\n",
    "number_of_trials = 100\n",
    "import os\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph2_'+str(name)+'_attr.npy'):\n",
    "        if len(analysis[(analysis['la'] == l)&(analysis['fa']==f)&(analysis['cl']==cl)&(analysis['asp']==asp)&(analysis['ad']==ad)] ) == 0:\n",
    "            data, train_indices, test_indices, train_mask, test_mask = data_load(name)\n",
    "            x = data.x.detach()\n",
    "            y = data.y.detach()\n",
    "            def objective(trial):\n",
    "            # Integer parameter\n",
    "                c = trial.suggest_categorical(\"c\",  [0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,20,30,100])\n",
    "                clf = LogisticRegression(max_iter = 3000, C=c).fit(x[train_mask].numpy(), y[train_mask].numpy())\n",
    "\n",
    "                accs_micro = []\n",
    "                accs_macro = []\n",
    "                for mask in [train_mask,test_mask,val_mask]:\n",
    "                    accs_micro += [f1_score(data.y.detach()[mask].numpy(),clf.predict(x[mask].numpy()), average='micro')]\n",
    "                    accs_macro += [f1_score(data.y.detach()[mask].numpy(),clf.predict(x[mask].numpy()), average='macro')]\n",
    "\n",
    "                return np.sqrt(accs_micro[2]*accs_macro[2])\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials = number_of_trials)\n",
    "            trial = study.best_trial\n",
    "            c=trial.params['c']\n",
    "            clf = LogisticRegression(max_iter = 3000, C=c).fit(x[train_mask].numpy(), y[train_mask].numpy())\n",
    "            accs_micro = []\n",
    "            accs_macro = []\n",
    "            for mask in [train_mask,test_mask,val_mask]:\n",
    "                accs_micro += [f1_score(y[mask].numpy(),clf.predict(x[mask].numpy()), average='micro')]\n",
    "                accs_macro += [f1_score(y[mask].numpy(),clf.predict(x[mask].numpy()), average='macro')]\n",
    "\n",
    "            to_append = pd.Series([l,f,cl,asp,ad, accs_micro[0],accs_micro[1], accs_macro[0] , accs_macro[1]],index = analysis.columns)\n",
    "            analysis = analysis.append(to_append, ignore_index=True)\n",
    "            analysis.to_csv('classification_on_features.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = Force2Vec\n",
    "loss_name = 'Force2Vec'\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "        print('hey')\n",
    "        for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name = name, conv = conv, device = device, loss_function = loss, mode = 'unsupervised')\n",
    "                best_values = MO.run(number_of_trials = 500)\n",
    "\n",
    "                loss_trgt = dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_force2vec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    " \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_SR\n",
    "loss_name = 'VERSE_SR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_PPR\n",
    "loss_name = 'VERSE_PPR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = LapEigen\n",
    "loss_name = 'LapEigen'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = GraphFactorization\n",
    "loss_name = 'GraphFactorization'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi, train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_AA\n",
    "loss_name = 'HOPE_AA'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_RPR\n",
    "loss_name = 'HOPE_RPR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_Katz\n",
    "loss_name = 'HOPE_Katz'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"betta\"] = best_values['betta']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = Node2Vec\n",
    "loss_name = 'Node2Vec'\n",
    "device = 'cpu'\n",
    "for name in ['chameleon']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                loss_trgt[\"p\"] = best_values['p']\n",
    "                loss_trgt[\"q\"] = best_values['q']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = DeepWalk\n",
    "loss_name = 'DeepWalk'\n",
    "device='cpu'\n",
    "for name in ['Citeseer']:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = APP\n",
    "loss_name = 'APP'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}