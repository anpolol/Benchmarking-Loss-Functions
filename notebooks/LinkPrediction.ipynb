{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch_geometric.datasets import Planetoid, WikipediaNetwork, Actor, WebKB\n",
    "\n",
    "from modules.model import Net\n",
    "from modules.sampling import SamplerContextMatrix, SamplerRandomWalk, SamplerFactorization, SamplerAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "synthetic = True\n",
    "benchmark_data_dir = \"../data_benchmark/\"\n",
    "help_data = \"../data_help/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "[4, 7, 9]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample([1,2,3,4,5,6,7,8,9],k=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "if synthetic:\n",
    "    datasets_names=[]\n",
    "    for l_a_trgt in [0.1,0.5,0.9]:\n",
    "                for f_a_trgt in [0.1,0.5,0.9]:\n",
    "                    for cl_trgt in [0.01,0.1,0.2,0.3,0.5]:\n",
    "                        for asp_trgt in [2,3,4,5,6,7]:\n",
    "                            for a_deg_trgt in [2,5,10,15,20,25,30,35,40]:\n",
    "                                datasets_names.append((l_a_trgt,f_a_trgt,cl_trgt,asp_trgt,a_deg_trgt))\n",
    "    def data_load(name):\n",
    "        x = torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_attr.npy'),dtype=torch.float)\n",
    "        edge_list = torch.tensor(np.load(f'{benchmark_data_dir}/graph_'+str(name)+'_edgelist.npy')).t()\n",
    "\n",
    "        data=Data(x=x,edge_index=edge_list)\n",
    "        indices=list(range(len(data.x)))\n",
    "\n",
    "        train_indices = random.sample(list(range(len(indices))), k = int(len(indices)*0.7))\n",
    "        train_indices_torch = torch.tensor(train_indices)\n",
    "\n",
    "        test_indices = list((set(indices)-set(train_indices)))\n",
    "        test_indices_torch = torch.tensor(test_indices)\n",
    "        train_mask = torch.tensor([False]*len(indices))\n",
    "        test_mask = torch.tensor([False]*len(indices))\n",
    "        train_mask[train_indices] = True\n",
    "        test_mask[test_indices]= True\n",
    "\n",
    "        return data, train_indices_torch,test_indices_torch,train_mask,test_mask\n",
    "else:\n",
    "    datasets_names = ['Cornell','Texas','Wisconsin','Actor','Pubmed','squirrel']\n",
    "\n",
    "    def data_load(name):\n",
    "        if name == 'Cora' or name == 'Citeseer' or name == 'Pubmed':\n",
    "            data = Planetoid(root='/tmp/'+str(name), name=name,transform=T.NormalizeFeatures())[0]\n",
    "        elif name == 'Actor':\n",
    "            data = Actor(root='/tmp/actor',transform=T.NormalizeFeatures())[0]\n",
    "        elif name == \"Cornell\" or name==\"Texas\" or name==\"Wisconsin\":\n",
    "            data = WebKB(root='/tmp/'+str(name),name=name,transform=T.NormalizeFeatures())[0]\n",
    "        elif name == 'squirrel' or name=='chameleon':\n",
    "            data = WikipediaNetwork(root='/tmp/'+str(name), name=name,transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "        indices=list(range(len(data.x)))\n",
    "\n",
    "        train_indices = torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "        val_indices = torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "        test_indices = torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        train_mask = torch.tensor([False]*len(indices))\n",
    "        test_mask = torch.tensor([False]*len(indices))\n",
    "        val_mask = torch.tensor([False]*len(indices))\n",
    "        train_mask[train_indices] =True\n",
    "        test_mask[test_indices]=True\n",
    "        val_mask[val_indices]=True\n",
    "        return data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "#loss functions\n",
    "\n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\":SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk_length\":[5, 10, 15, 20],\"walks_per_node\":[5, 10, 15, 20],\"num_negative_samples\":[1,6, 11, 16, 21],\"context_size\" : [5, 10, 15, 20],\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\" : SamplerRandomWalk } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk_length\":[5, 10, 15, 20],\"walks_per_node\":[5, 10, 15, 20],\"num_negative_samples\":[1,6, 11, 16, 21],\"context_size\" : [5, 10, 15, 20],\"p\": [0.25, 0.50, 1, 2, 4] ,\"q\":[0.25, 0.50, 1, 2, 4], \"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\": SamplerRandomWalk}#то же самое\n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerAPP}\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"betta\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "\n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\", \"C\":\"CN\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "\n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "\n",
    "Force2Vec = {\"Name\": \"Force2Vec\",\"C\": \"Adj\",\"num_negative_samples\":[1, 6, 11, 16, 21],\"loss var\": \"Force2Vec\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix,\"lmbda\": [0.0,1.0]}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from modules.negativeSampling import NegativeSampler\n",
    "\n",
    "class Main:\n",
    "    def __init__(self,name, conv, device, loss_function, mode):\n",
    "        data, train_indices,test_indices,train_mask,test_mask = data_load(name)\n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.x = data.x\n",
    "        self.data=data.to(device)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.datasetname=name\n",
    "        self.train_indices =train_indices# torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "\n",
    "        self.test_indices = test_indices#torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        self.train_mask = train_mask#torch.tensor([False]*len(indices))\n",
    "        self.test_mask = test_mask#torch.tensor([False]*len(indices))\n",
    "\n",
    "        self.flag = self.loss[\"flag_tosave\"]\n",
    "        super(Main, self).__init__()\n",
    "    def sampling(self,Sampler, epoch, nodes, loss,train_flag = True):\n",
    "            if (epoch == 0):\n",
    "                if self.flag:\n",
    "                    if \"alpha\" in self.loss:\n",
    "                        name_of_file = self.datasetname+\"_samples_\"+loss[\"Name\"]+\"_alpha_\"+str(loss[\"alpha\"])+\".pickle\"\n",
    "                    elif \"betta\" in self.loss:\n",
    "                        name_of_file = self.datasetname+\"_samples_\"+loss[\"Name\"]+\"_betta_\"+str(loss[\"betta\"])+\".pickle\"\n",
    "                    else:\n",
    "                        name_of_file = self.datasetname+\"_samples_\"+loss[\"Name\"]+\".pickle\"\n",
    "\n",
    "                    if os.path.exists(f'{help_data}/'+str(name_of_file)):\n",
    "                        with open(f'{help_data}/'+str(name_of_file),'rb') as f:\n",
    "                            self.samples = pickle.load(f)\n",
    "                    else:\n",
    "                        self.samples = Sampler.sample(nodes)\n",
    "                        with open(f'{help_data}/'+str(name_of_file),'wb') as f:\n",
    "                            pickle.dump(self.samples,f)\n",
    "                else:\n",
    "                    self.samples = Sampler.sample(nodes,train_flag)\n",
    " \n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch,loss):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "       # print('train loader',len(train_loader))\n",
    "        \n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr = torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                #print('before',data.x)\n",
    "                out = model.inference(data.to(self.device),dp=dropout)\n",
    "                #print('after',out, sum(sum(out)))\n",
    "\n",
    "                samples = self.sampling(Sampler,epoch, indices_of_train_data,loss,train_flag=True)\n",
    "                loss = model.loss(out[self.train_mask], self.samples)\n",
    "                #print('loss',loss)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(self.device)].to(self.device), adjs)\n",
    "\n",
    "                    self.sampling(Sampler,epoch,n_id[:batch_size],loss,train_flag=True)\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader), out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model, data,test_loader,epoch,Sampler,loss):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr = torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                #print('before',data.x)\n",
    "                out = model.inference(data.to(self.device),dp=0)\n",
    "                #print('after',out, sum(sum(out)))\n",
    "                samples = self.sampling(Sampler,epoch, indices_of_train_data,loss,train_flag=False)\n",
    "                loss = model.loss(out[self.test_mask], self.samples)\n",
    "                #print('loss',loss)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in test_loader:\n",
    "                    if len(test_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(self.device)].to(self.device), adjs)\n",
    "\n",
    "                    self.sampling(Sampler,epoch,n_id[:batch_size],loss,train_flag=False)\n",
    "\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "\n",
    "            return total_loss /len(test_loader)\n",
    "\n",
    "    def run(self,params):\n",
    "        hidden_layer=params['hidden_layer']\n",
    "        out_layer=params['out_layer']\n",
    "        dropout=params['dropout']\n",
    "        size=params['size of network, number of convs']\n",
    "        learning_rate=params['lr']\n",
    "        #hidden_layer_for_classifier=params['hidden_layer_for_classifier']\n",
    "        #alpha_for_classifier = params['alpha_for_classifier']\n",
    "        #learning_rate_for_classifier = params['learning_rate_for_classifier']\n",
    "        #n_layers_for_classifier = params['n_layers_for_classifier']\n",
    "\n",
    "        #hidden_layer=64,out_layer=128,dropout=0.0,size=1,learning_rate=0.001,c=100\n",
    "        classifier = \"logistic regression\"\n",
    "        self.data.edge_index= self.data.edge_index.type(torch.LongTensor)\n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=torch.BoolTensor([True]*len(self.data.x)), batch_size = int(len(self.data.x)), sizes=[-1]*size)\n",
    "        \n",
    "        Sampler = self.loss[\"Sampler\"]\n",
    "        LossSampler = Sampler(self.datasetname, self.data, device=device, mask=torch.BoolTensor([True]*len(self.data.x)), loss_info=self.loss, help_dir=help_data)\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(self.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        for epoch in range(100):\n",
    "                    loss,out = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch,self.loss)\n",
    "\n",
    "        np.save(str(self.datasetname)+'_'+ str(self.loss['Name'])+'_emb.npy', out.detach().cpu().numpy())\n",
    "\n",
    "        positive_edges = []\n",
    "        for edge in self.data.edge_index.T.tolist():\n",
    "            if edge[0] in self.train_indices:\n",
    "                if edge[1] in self.train_indices:\n",
    "                    positive_edges.append(edge)\n",
    "\n",
    "        ns = NegativeSampler(data = self.data)\n",
    "        num_negative_samples = int(len(positive_edges)/len(self.train_indices))\n",
    "        neg_samples = ns.negative_sampling( self.train_indices, num_negative_samples=num_negative_samples)\n",
    "\n",
    "        # find treshold\n",
    "        max_acc = 0\n",
    "        max_acc_k = 0\n",
    "\n",
    "        true = [1]*len(positive_edges)+[0]*len(neg_samples)\n",
    "        emb_norm = torch.nn.functional.normalize(torch.tensor(out.detach().cpu()))\n",
    "        for k in np.linspace(0,1,100).tolist():\n",
    "            pred = []\n",
    "            for edge in positive_edges:\n",
    "                pred.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])).tolist() > k)\n",
    "            for edge in neg_samples:\n",
    "                pred.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])).tolist() > k)\n",
    "            acc = accuracy_score(true,pred)\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                max_acc_k = k\n",
    "\n",
    "        positive_edges_test = []\n",
    "        for edge in self.data.edge_index.T.tolist():\n",
    "            if edge[0] in self.test_indices:\n",
    "                if edge[1] in self.test_indices:\n",
    "                    positive_edges_test.append(edge)\n",
    "\n",
    "        num_neg_samples_test = int(len(positive_edges)/len(self.test_indices))\n",
    "        ns = NegativeSampler(data=self.data)\n",
    "        neg_samples_test = ns.negative_sampling( self.test_indices, num_negative_samples=num_neg_samples_test)\n",
    "        pred_test=[]\n",
    "        for edge in positive_edges_test:\n",
    "            pred_test.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])) > max_acc_k)\n",
    "        #print(torch.sigmoid(torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])))\n",
    "        for edge in neg_samples_test:\n",
    "            pred_test.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])) > max_acc_k)\n",
    "\n",
    "        true_test = [1]*len(positive_edges_test)+[0]*len(neg_samples_test)\n",
    "\n",
    "        return accuracy_score(true_test, pred_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-3,1e-2)\n",
    "     #   learning_rate_for_classifier =trial.suggest_float(\"learning_rate_for_classifier\",5e-3,1e-2)\n",
    "      #  n_layers_for_classifier = trial.suggest_categorical(\"n_layers_for_classifier\", [1,2,3])\n",
    "       # alpha_for_classifier = trial.suggest_categorical(\"alpha_for_classifier\",  [0.001, 0.01, 0.1,0.3,0.5,0.7,0.9,1,10,20,30,100])\n",
    "        #c =trial.suggest_categorical(\"c\",  [0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,20,30,100])\n",
    "        #hidden_layer_for_classifier = trial.suggest_categorical(\"hidden_layer_for_classifier\", [32,64,128,256])\n",
    "        # варьируем параметры\n",
    "        loss_to_train={}\n",
    "        for name in self.loss:\n",
    "            \n",
    "            if type(self.loss[name]) == list :\n",
    "                if len(self.loss[name]) == 3:\n",
    "                    var = trial.suggest_int(name,self.loss[name][0],self.loss[name][1],step=self.loss[name][2])\n",
    "                    loss_to_train[name] = var\n",
    "                elif len(self.loss[name]) == 2:\n",
    "                    var_2 = trial.suggest_float(name,self.loss[name][0],self.loss[name][1])\n",
    "                    loss_to_train[name] = var_2\n",
    "                else:\n",
    "                    var_3 = trial.suggest_categorical(name, self.loss[name])\n",
    "                    loss_to_train[name] = var_3\n",
    "            else:\n",
    "                loss_to_train[name] = self.loss[name]\n",
    "        if name =='q' and type(self.loss[name]) == list:\n",
    "            var_5 = trial.suggest_categorical('p', self.loss['p'])\n",
    "            var_4 = trial.suggest_categorical('q', self.loss[name])\n",
    "            if var_4 > 1:\n",
    "                var_4=1\n",
    "            if var_5 < var_4:     \n",
    "                var_5=var_4\n",
    "            loss_to_train['q'] = var_4\n",
    "            loss_to_train['p'] = var_5\n",
    "                \n",
    "        Sampler =loss_to_train[\"Sampler\"]\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=Conv,loss_function=loss_to_train,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "        self.data.edge_index= self.data.edge_index.type(torch.LongTensor)\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "        test_loader = NeighborSampler(self.data.edge_index,  batch_size = int(sum(self.test_mask)),node_idx=self.test_mask, sizes=[-1]*size)\n",
    "        LossSampler = Sampler(self.datasetname,self.data,device=self.device,mask=self.train_mask,loss_info=loss_to_train, help_dir=help_data)\n",
    "        LossSamplerTest = Sampler(self.datasetname,self.data,device=self.device,mask=self.test_mask,loss_info=loss_to_train, help_dir=help_data)\n",
    "        model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        for epoch in range(50):\n",
    "            loss,_ = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch,loss_to_train)\n",
    "        loss_test = self.test(model=model, data=self.data, epoch=0, test_loader=test_loader,Sampler=LossSamplerTest,loss=loss_to_train)\n",
    "        trial.report(loss_test, epoch)\n",
    "        return loss_test\n",
    "\n",
    "    def run(self,number_of_trials):\n",
    "        study = optuna.create_study(direction=\"minimize\",study_name=self.loss[\"Name\"]+\" loss,\"+str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "        trial = study.best_trial\n",
    "        return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "best_values={'hidden_layer': 64,\n",
    " 'out_layer': 64,\n",
    " 'dropout': 0.2,\n",
    " 'size of network, number of convs': 3,\n",
    " 'lr': 0.009743826238823094,\n",
    " 'num_negative_samples': 16,\n",
    " 'lmbda': 0.7358704799534493}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "data, train_indices, test_indices, train_mask, test_mask = data_load('random')\n",
    "data.edge_index= data.edge_index.type(torch.LongTensor)\n",
    "edges = data.edge_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "positive_edges = []\n",
    "for edge in edges.T.tolist():\n",
    "    positive_edges.append(edge)\n",
    "\n",
    "num_neg_samples = int(len(positive_edges)/100)\n",
    "from modules.negativeSampling import NegativeSampler\n",
    "ns = NegativeSampler(data=data)\n",
    "neg_samples = ns.negative_sampling( torch.tensor(list(range(100)), dtype=torch.long) , num_negative_samples=num_neg_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "800"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "true=[1]*len(positive_edges)+[-1]*len(neg_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0438, -0.1068, -0.1145,  ..., -0.0959,  0.0472, -0.0773],\n        [-0.0438, -0.1068, -0.1145,  ..., -0.0959,  0.0472, -0.0773],\n        [-0.0438, -0.1068, -0.1145,  ..., -0.0959,  0.0472, -0.0773],\n        ...,\n        [-0.0439, -0.0991, -0.1091,  ..., -0.1312,  0.0344, -0.1085],\n        [-0.0214, -0.0489, -0.0714,  ..., -0.1348,  0.0360, -0.1409],\n        [-0.0412, -0.0461, -0.0935,  ..., -0.1703,  0.0223, -0.1446]])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.load('randomLINE_emb.npy')\n",
    "emb_norm = torch.nn.functional.normalize(torch.tensor(emb))\n",
    "emb_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6178343949044586"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_edges_test = []\n",
    "for edge in edges.T.tolist():\n",
    "    if edge[0] in range(71,100):\n",
    "        if edge[1] in range(71,100):\n",
    "            positive_edges_test.append(edge)\n",
    "\n",
    "num_neg_samples_test = int(len(positive_edges)/30)\n",
    "from modules.negativeSampling import NegativeSampler\n",
    "ns = NegativeSampler(data=data)\n",
    "neg_samples_test = ns.negative_sampling( torch.tensor(list(range(71,100)), dtype=torch.long) , num_negative_samples=num_neg_samples)\n",
    "pred_test=[]\n",
    "for edge in positive_edges_test:\n",
    "    pred_test.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])) > max_acc_k)\n",
    "#print(torch.sigmoid(torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])))\n",
    "for edge in neg_samples_test:\n",
    "    pred_test.append((torch.dot(emb_norm[edge[0]],emb_norm[edge[1]])) > max_acc_k)\n",
    "\n",
    "true_test = [1]*len(positive_edges_test)+[0]*len(neg_samples_test)\n",
    "accuracy_score(true_test, pred_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6666666666666666"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_loss_k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16896\\2429962292.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mk\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mParameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.001\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mweight_decay\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1e-5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\optim\\adam.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable)\u001B[0m\n\u001B[0;32m     88\u001B[0m                         \u001B[0mweight_decay\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mweight_decay\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mamsgrad\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m                         maximize=maximize, foreach=foreach, capturable=capturable)\n\u001B[1;32m---> 90\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdefaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__setstate__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, params, defaults)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mparam_group\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mparam_groups\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_param_group\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam_group\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     55\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[1;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001B[0m in \u001B[0;36madd_param_group\u001B[1;34m(self, param_group)\u001B[0m\n\u001B[0;32m    294\u001B[0m                                 \"but one of the params is \" + torch.typename(param))\n\u001B[0;32m    295\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mparam\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_leaf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 296\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"can't optimize a non-leaf Tensor\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    297\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdefault\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdefaults\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "k = iter(torch.nn.parameter.Parameter(torch.Tensor(1)))\n",
    "optimizer = torch.optim.Adam(k, lr=0.001,weight_decay = 1e-5)\n",
    "print(k)\n",
    "optimizer.zero_grad()\n",
    "loss = 0\n",
    "for edge in positive_edges:\n",
    "    loss = - (torch.dot(emb[edge[0]],emb[edge[1]]) > k)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 12:04:47,855]\u001B[0m A new study created in memory with name: LINE loss,GCN conv\u001B[0m\n",
      "\u001B[33m[W 2023-01-24 12:04:50,239]\u001B[0m Trial 0 failed because of the following error: RuntimeError('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.')\u001B[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13940\\3469056074.py\", line 54, in objective\n",
      "    loss_test = self.test(model=model, data=self.data, epoch=0, test_loader=test_loader,Sampler=LossSamplerTest,loss=loss_to_train)\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13940\\1923938601.py\", line 87, in test\n",
      "    loss = model.loss(out[self.test_mask], self.samples)\n",
      "  File \"C:\\Users\\User\\Desktop\\Benchmarking-Loss-Functions\\modules\\model.py\", line 170, in lossContextMatrix\n",
      "    h_rest = out[rest].view(rest.shape[0], -1, self.out_layer)\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13940\\2457124506.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresults_df\u001B[0m\u001B[1;33m[\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mresults_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'loss'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mloss_name\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m&\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mresults_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'conv'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mconv\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m&\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mresults_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dataset'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m                     \u001B[0mMO\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMainOptuna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmode\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'unsupervised'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m                     \u001B[0mbest_values\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mMO\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnumber_of_trials\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m500\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m                     \u001B[0mloss_trgt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13940\\3469056074.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, number_of_trials)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnumber_of_trials\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m         \u001B[0mstudy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptuna\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreate_study\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirection\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"minimize\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mstudy_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Name\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m\" loss,\"\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mConv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m\" conv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m         \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobjective\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mn_trials\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnumber_of_trials\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m         \u001B[0mtrial\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_trial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtrial\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\study.py\u001B[0m in \u001B[0;36moptimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    426\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    427\u001B[0m             \u001B[0mgc_after_trial\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgc_after_trial\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 428\u001B[1;33m             \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    429\u001B[0m         )\n\u001B[0;32m    430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     74\u001B[0m                 \u001B[0mreseed_sampler_rng\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m                 \u001B[0mtime_start\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m                 \u001B[0mprogress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprogress_bar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m             )\n\u001B[0;32m     78\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 160\u001B[1;33m             \u001B[0mfrozen_trial\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_run_trial\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstudy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    161\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m             \u001B[1;31m# The following line mitigates memory problems that can be occurred in some\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    232\u001B[0m         \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc_err\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    233\u001B[0m     ):\n\u001B[1;32m--> 234\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mfunc_err\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    235\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mfrozen_trial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    194\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mget_heartbeat_thread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_trial_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_storage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    195\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 196\u001B[1;33m             \u001B[0mvalue_or_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    197\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mexceptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrialPruned\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m             \u001B[1;31m# TODO(mamu): Handle multi-objective cases.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13940\\3469056074.py\u001B[0m in \u001B[0;36mobjective\u001B[1;34m(self, trial)\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m50\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m             \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mLossSampler\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_to_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m         \u001B[0mloss_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mSampler\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mLossSamplerTest\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mloss_to_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     55\u001B[0m         \u001B[0mtrial\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreport\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mloss_test\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13940\\1923938601.py\u001B[0m in \u001B[0;36mtest\u001B[1;34m(self, model, data, test_loader, epoch, Sampler, loss)\u001B[0m\n\u001B[0;32m     85\u001B[0m                 \u001B[1;31m#print('after',out, sum(sum(out)))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m                 \u001B[0msamples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msampling\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSampler\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices_of_train_data\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_flag\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest_mask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m                 \u001B[1;31m#print('loss',loss)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m                 \u001B[0mtotal_loss\u001B[0m\u001B[1;33m+=\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\Benchmarking-Loss-Functions\\modules\\model.py\u001B[0m in \u001B[0;36mlossContextMatrix\u001B[1;34m(self, out, PosNegSamples)\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[0mrest\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrest\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    169\u001B[0m         \u001B[1;31m# print('neg',start,rest)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 170\u001B[1;33m         \u001B[0mh_rest\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mrest\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mout_layer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m         \u001B[0mdot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mh_start\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mh_rest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "results_df = pd.DataFrame(columns=['loss','conv','fa', 'cl', 'asp','ad','dataset','acc lp','best_values'])\n",
    "device =torch.device('cuda')\n",
    "\n",
    "for conv in ['GCN']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names[810:1620]:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(results_df[ (results_df['loss'] == loss_name) & (results_df['conv'] == conv) & (results_df['dataset'] == name)] ) == 0:\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function = loss,mode = 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "                    loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    accuracy = M.run(best_values)\n",
    "\n",
    "                    to_append=pd.Series([loss_name, conv, f,cl,asp,ad,name , accuracy, best_values],index = results_df.columns)\n",
    "                    results_df = results_df.append(to_append,ignore_index=True)\n",
    "                    results_df.to_csv('results_on_LP.csv')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "     loss  conv   fa    cl asp  ad        dataset    acc lp  \\\n0    LINE  SAGE  0.1  0.01   2   5   0.50.10.0125  0.646192   \n1    LINE  SAGE  0.1  0.01   2  20  0.50.10.01220  0.483070   \n2    LINE  SAGE  0.1  0.01   2  40  0.50.10.01240  0.602662   \n3    LINE  SAGE  0.1  0.01   3   2   0.50.10.0132  0.649907   \n4    LINE  SAGE  0.1  0.01   3   5   0.50.10.0135  0.586957   \n..    ...   ...  ...   ...  ..  ..            ...       ...   \n133  LINE   GAT  0.9  0.50   2  20   0.50.90.5220  0.615674   \n134  LINE   GAT  0.9  0.50   2  40   0.50.90.5240  0.885456   \n135  LINE   GAT  0.9  0.50   3   5    0.50.90.535  0.928410   \n136  LINE   GAT  0.9  0.50   3  20   0.50.90.5320  0.699322   \n137  LINE   GAT  0.9  0.50   4   5    0.50.90.545  0.960060   \n\n                                           best_values  \n0    {'hidden_layer': 64, 'out_layer': 128, 'dropou...  \n1    {'hidden_layer': 64, 'out_layer': 32, 'dropout...  \n2    {'hidden_layer': 64, 'out_layer': 32, 'dropout...  \n3    {'hidden_layer': 256, 'out_layer': 32, 'dropou...  \n4    {'hidden_layer': 64, 'out_layer': 32, 'dropout...  \n..                                                 ...  \n133  {'hidden_layer': 128, 'out_layer': 32, 'dropou...  \n134  {'hidden_layer': 128, 'out_layer': 32, 'dropou...  \n135  {'hidden_layer': 128, 'out_layer': 32, 'dropou...  \n136  {'hidden_layer': 128, 'out_layer': 32, 'dropou...  \n137  {'hidden_layer': 256, 'out_layer': 32, 'dropou...  \n\n[138 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>conv</th>\n      <th>fa</th>\n      <th>cl</th>\n      <th>asp</th>\n      <th>ad</th>\n      <th>dataset</th>\n      <th>acc lp</th>\n      <th>best_values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LINE</td>\n      <td>SAGE</td>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.50.10.0125</td>\n      <td>0.646192</td>\n      <td>{'hidden_layer': 64, 'out_layer': 128, 'dropou...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LINE</td>\n      <td>SAGE</td>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>20</td>\n      <td>0.50.10.01220</td>\n      <td>0.483070</td>\n      <td>{'hidden_layer': 64, 'out_layer': 32, 'dropout...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LINE</td>\n      <td>SAGE</td>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>40</td>\n      <td>0.50.10.01240</td>\n      <td>0.602662</td>\n      <td>{'hidden_layer': 64, 'out_layer': 32, 'dropout...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LINE</td>\n      <td>SAGE</td>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0.50.10.0132</td>\n      <td>0.649907</td>\n      <td>{'hidden_layer': 256, 'out_layer': 32, 'dropou...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LINE</td>\n      <td>SAGE</td>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.50.10.0135</td>\n      <td>0.586957</td>\n      <td>{'hidden_layer': 64, 'out_layer': 32, 'dropout...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>LINE</td>\n      <td>GAT</td>\n      <td>0.9</td>\n      <td>0.50</td>\n      <td>2</td>\n      <td>20</td>\n      <td>0.50.90.5220</td>\n      <td>0.615674</td>\n      <td>{'hidden_layer': 128, 'out_layer': 32, 'dropou...</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>LINE</td>\n      <td>GAT</td>\n      <td>0.9</td>\n      <td>0.50</td>\n      <td>2</td>\n      <td>40</td>\n      <td>0.50.90.5240</td>\n      <td>0.885456</td>\n      <td>{'hidden_layer': 128, 'out_layer': 32, 'dropou...</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>LINE</td>\n      <td>GAT</td>\n      <td>0.9</td>\n      <td>0.50</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.50.90.535</td>\n      <td>0.928410</td>\n      <td>{'hidden_layer': 128, 'out_layer': 32, 'dropou...</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>LINE</td>\n      <td>GAT</td>\n      <td>0.9</td>\n      <td>0.50</td>\n      <td>3</td>\n      <td>20</td>\n      <td>0.50.90.5320</td>\n      <td>0.699322</td>\n      <td>{'hidden_layer': 128, 'out_layer': 32, 'dropou...</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>LINE</td>\n      <td>GAT</td>\n      <td>0.9</td>\n      <td>0.50</td>\n      <td>4</td>\n      <td>5</td>\n      <td>0.50.90.545</td>\n      <td>0.960060</td>\n      <td>{'hidden_layer': 256, 'out_layer': 32, 'dropou...</td>\n    </tr>\n  </tbody>\n</table>\n<p>138 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: SAGE, mode: unsupervised, loss from LINE\n",
      "0\n",
      "Loss: 13.8690, Epoch: 000\n",
      "1\n",
      "Loss: 39.4679, Epoch: 001\n",
      "2\n",
      "Loss: 2.8382, Epoch: 002\n",
      "3\n",
      "Loss: 2.5351, Epoch: 003\n",
      "4\n",
      "Loss: 2.5282, Epoch: 004\n",
      "5\n",
      "Loss: 2.5177, Epoch: 005\n",
      "6\n",
      "Loss: 2.4958, Epoch: 006\n",
      "7\n",
      "Loss: 2.4682, Epoch: 007\n",
      "8\n",
      "Loss: 2.4460, Epoch: 008\n",
      "9\n",
      "Loss: 2.4131, Epoch: 009\n",
      "10\n",
      "Loss: 2.3725, Epoch: 010\n",
      "11\n",
      "Loss: 2.3376, Epoch: 011\n",
      "12\n",
      "Loss: 2.3069, Epoch: 012\n",
      "13\n",
      "Loss: 2.3199, Epoch: 013\n",
      "14\n",
      "Loss: 2.2596, Epoch: 014\n",
      "15\n",
      "Loss: 2.2247, Epoch: 015\n",
      "16\n",
      "Loss: 2.2191, Epoch: 016\n",
      "17\n",
      "Loss: 2.1942, Epoch: 017\n",
      "18\n",
      "Loss: 2.1849, Epoch: 018\n",
      "19\n",
      "Loss: 2.1633, Epoch: 019\n",
      "20\n",
      "Loss: 2.1456, Epoch: 020\n",
      "21\n",
      "Loss: 2.1660, Epoch: 021\n",
      "22\n",
      "Loss: 2.1352, Epoch: 022\n",
      "23\n",
      "Loss: 2.1367, Epoch: 023\n",
      "24\n",
      "Loss: 2.1370, Epoch: 024\n",
      "25\n",
      "Loss: 2.1201, Epoch: 025\n",
      "26\n",
      "Loss: 2.1071, Epoch: 026\n",
      "27\n",
      "Loss: 2.1096, Epoch: 027\n",
      "28\n",
      "Loss: 2.1104, Epoch: 028\n",
      "29\n",
      "Loss: 2.1074, Epoch: 029\n",
      "30\n",
      "Loss: 2.1058, Epoch: 030\n",
      "31\n",
      "Loss: 2.0986, Epoch: 031\n",
      "32\n",
      "Loss: 2.1012, Epoch: 032\n",
      "33\n",
      "Loss: 2.0985, Epoch: 033\n",
      "34\n",
      "Loss: 2.1062, Epoch: 034\n",
      "35\n",
      "Loss: 2.0988, Epoch: 035\n",
      "36\n",
      "Loss: 2.1027, Epoch: 036\n",
      "37\n",
      "Loss: 2.1102, Epoch: 037\n",
      "38\n",
      "Loss: 2.0938, Epoch: 038\n",
      "39\n",
      "Loss: 2.1069, Epoch: 039\n",
      "40\n",
      "Loss: 2.0908, Epoch: 040\n",
      "41\n",
      "Loss: 2.0894, Epoch: 041\n",
      "42\n",
      "Loss: 2.0967, Epoch: 042\n",
      "43\n",
      "Loss: 2.0928, Epoch: 043\n",
      "44\n",
      "Loss: 2.0998, Epoch: 044\n",
      "45\n",
      "Loss: 2.1076, Epoch: 045\n",
      "46\n",
      "Loss: 2.0944, Epoch: 046\n",
      "47\n",
      "Loss: 2.0875, Epoch: 047\n",
      "48\n",
      "Loss: 2.0827, Epoch: 048\n",
      "49\n",
      "Loss: 2.0850, Epoch: 049\n",
      "50\n",
      "Loss: 2.0828, Epoch: 050\n",
      "51\n",
      "Loss: 2.1182, Epoch: 051\n",
      "52\n",
      "Loss: 2.0918, Epoch: 052\n",
      "53\n",
      "Loss: 2.0862, Epoch: 053\n",
      "54\n",
      "Loss: 2.0872, Epoch: 054\n",
      "55\n",
      "Loss: 2.0969, Epoch: 055\n",
      "56\n",
      "Loss: 2.0927, Epoch: 056\n",
      "57\n",
      "Loss: 2.0932, Epoch: 057\n",
      "58\n",
      "Loss: 2.0911, Epoch: 058\n",
      "59\n",
      "Loss: 2.0948, Epoch: 059\n",
      "60\n",
      "Loss: 2.0914, Epoch: 060\n",
      "61\n",
      "Loss: 2.0852, Epoch: 061\n",
      "62\n",
      "Loss: 2.0897, Epoch: 062\n",
      "63\n",
      "Loss: 2.0866, Epoch: 063\n",
      "64\n",
      "Loss: 2.0923, Epoch: 064\n",
      "65\n",
      "Loss: 2.0833, Epoch: 065\n",
      "66\n",
      "Loss: 2.0841, Epoch: 066\n",
      "67\n",
      "Loss: 2.0896, Epoch: 067\n",
      "68\n",
      "Loss: 2.0809, Epoch: 068\n",
      "69\n",
      "Loss: 2.0828, Epoch: 069\n",
      "70\n",
      "Loss: 2.0822, Epoch: 070\n",
      "71\n",
      "Loss: 2.0815, Epoch: 071\n",
      "72\n",
      "Loss: 2.0809, Epoch: 072\n",
      "73\n",
      "Loss: 2.0803, Epoch: 073\n",
      "74\n",
      "Loss: 2.0809, Epoch: 074\n",
      "75\n",
      "Loss: 2.0814, Epoch: 075\n",
      "76\n",
      "Loss: 2.0895, Epoch: 076\n",
      "77\n",
      "Loss: 2.0816, Epoch: 077\n",
      "78\n",
      "Loss: 2.0791, Epoch: 078\n",
      "79\n",
      "Loss: 2.0814, Epoch: 079\n",
      "80\n",
      "Loss: 2.0826, Epoch: 080\n",
      "81\n",
      "Loss: 2.0825, Epoch: 081\n",
      "82\n",
      "Loss: 2.0847, Epoch: 082\n",
      "83\n",
      "Loss: 2.0805, Epoch: 083\n",
      "84\n",
      "Loss: 2.0811, Epoch: 084\n",
      "85\n",
      "Loss: 2.0792, Epoch: 085\n",
      "86\n",
      "Loss: 2.0807, Epoch: 086\n",
      "87\n",
      "Loss: 2.0821, Epoch: 087\n",
      "88\n",
      "Loss: 2.0817, Epoch: 088\n",
      "89\n",
      "Loss: 2.0806, Epoch: 089\n",
      "90\n",
      "Loss: 2.0880, Epoch: 090\n",
      "91\n",
      "Loss: 2.0803, Epoch: 091\n",
      "92\n",
      "Loss: 2.0808, Epoch: 092\n",
      "93\n",
      "Loss: 2.0795, Epoch: 093\n",
      "94\n",
      "Loss: 2.0792, Epoch: 094\n",
      "95\n",
      "Loss: 2.0793, Epoch: 095\n",
      "96\n",
      "Loss: 2.0800, Epoch: 096\n",
      "97\n",
      "Loss: 2.0797, Epoch: 097\n",
      "98\n",
      "Loss: 2.0818, Epoch: 098\n",
      "99\n",
      "Loss: 2.0804, Epoch: 099\n",
      "Loss: 2.0804, Epoch: 099\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_values={'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 3, 'lr': 0.008061289273891234, 'num_negative_samples': 1, 'lmbda': 0.48186987089486877}\n",
    "loss_trgt=LINE\n",
    "loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "M.run(best_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "analysis = pd.read_csv('../results/classification_with_MLP.csv')\n",
    "analysis = analysis.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "for conv in ['SAGE','GCN','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "                    loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                    to_append=pd.Series([loss_name, conv, l,f,cl,asp,ad,name,train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,best_values],index = analysis.columns)\n",
    "                    analysis = analysis.append(to_append,ignore_index=True)\n",
    "                    #analysis.to_csv('../results/classification_with_MLP.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "analysis = pd.read_csv('../results/classification_with_MLP.csv')\n",
    "analysis = analysis.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "for conv in ['SAGE','GCN','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                    to_append=pd.Series([loss_name, conv, l,f,cl,asp,ad,name,train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,best_values],index = analysis.columns)\n",
    "                    analysis = analysis.append(to_append,ignore_index=True)\n",
    "                    analysis.to_csv('../results/classification_with_MLP.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_with_MLP.csv')\n",
    "analysis = analysis.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "for conv in ['SAGE','GCN','GAT']:\n",
    "    for (l,f,cl,asp,ad) in datasets_names:\n",
    "        name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "        if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "                if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                    MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                    best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                    loss_trgt=dict()\n",
    "                    for par in loss:\n",
    "                        loss_trgt[par]=loss[par]\n",
    "\n",
    "                    loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                    loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                    M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                    train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "                    to_append=pd.Series([loss_name, conv, l,f,cl,asp,ad,name,train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma,best_values],index = analysis.columns)\n",
    "                    analysis = analysis.append(to_append,ignore_index=True)\n",
    "                    analysis.to_csv('../results/classification_with_MLP.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ниже надо еще редактировать"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LapEigen\n",
    "loss_name = 'LapEigen'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "analysis=analysis.drop(columns=['Unnamed: 0'])\n",
    "for name in datasets_names[3:4]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = Node2Vec\n",
    "loss_name = 'Node2Vec'\n",
    "device= 'cpu'\n",
    "for name in ['Cornell']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                loss_trgt[\"p\"] = best_values['p']\n",
    "                loss_trgt[\"q\"] = best_values['q']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_PPR\n",
    "loss_name = 'VERSE_PPR'\n",
    "\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "analysis = pd.read_csv('../results/classification_catboost.csv')\n",
    "for name in datasets_names[:2]:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#on real graphs\n",
    "loss = Force2Vec\n",
    "loss_name = 'Force2Vec'\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "for name in datasets_names[2:]:\n",
    "    for conv in ['GCN']:\n",
    "         if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                MO = MainOptuna(name = name, conv = conv, device = device, loss_function = loss, mode = 'unsupervised')\n",
    "                best_values = MO.run(number_of_trials =500)\n",
    "\n",
    "                loss_trgt = dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name, conv=conv, device=device, loss_function=loss_trgt, mode='unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = VERSE_SR\n",
    "loss_name = 'VERSE_SR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('../results/classification_catboost.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modkdjfjf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for (l,f,cl,asp,ad) in datasets_names:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13608\\3748997711.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[1;34m\"\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;33m[\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcl\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0masp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mad\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'../data_benchmark/graph2_'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m'_attr.npy'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'la'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0ml\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m&\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'fa'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m==\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m&\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'cl'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m==\u001B[0m\u001B[0mcl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m&\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'asp'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m==\u001B[0m\u001B[0masp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m&\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manalysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'ad'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m==\u001B[0m\u001B[0mad\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m             \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_indices\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_indices\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_mask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m             \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'\n",
    "number_of_trials = 100\n",
    "import os\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph2_'+str(name)+'_attr.npy'):\n",
    "        if len(analysis[(analysis['la'] == l)&(analysis['fa']==f)&(analysis['cl']==cl)&(analysis['asp']==asp)&(analysis['ad']==ad)] ) == 0:\n",
    "            data, train_indices, test_indices, train_mask, test_mask = data_load(name)\n",
    "            x = data.x.detach()\n",
    "            y = data.y.detach()\n",
    "            def objective(trial):\n",
    "            # Integer parameter\n",
    "                c = trial.suggest_categorical(\"c\",  [0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10,20,30,100])\n",
    "                clf = LogisticRegression(max_iter = 3000, C=c).fit(x[train_mask].numpy(), y[train_mask].numpy())\n",
    "\n",
    "                accs_micro = []\n",
    "                accs_macro = []\n",
    "                for mask in [train_mask,test_mask,val_mask]:\n",
    "                    accs_micro += [f1_score(data.y.detach()[mask].numpy(),clf.predict(x[mask].numpy()), average='micro')]\n",
    "                    accs_macro += [f1_score(data.y.detach()[mask].numpy(),clf.predict(x[mask].numpy()), average='macro')]\n",
    "\n",
    "                return np.sqrt(accs_micro[2]*accs_macro[2])\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials = number_of_trials)\n",
    "            trial = study.best_trial\n",
    "            c=trial.params['c']\n",
    "            clf = LogisticRegression(max_iter = 3000, C=c).fit(x[train_mask].numpy(), y[train_mask].numpy())\n",
    "            accs_micro = []\n",
    "            accs_macro = []\n",
    "            for mask in [train_mask,test_mask,val_mask]:\n",
    "                accs_micro += [f1_score(y[mask].numpy(),clf.predict(x[mask].numpy()), average='micro')]\n",
    "                accs_macro += [f1_score(y[mask].numpy(),clf.predict(x[mask].numpy()), average='macro')]\n",
    "\n",
    "            to_append = pd.Series([l,f,cl,asp,ad, accs_micro[0],accs_micro[1], accs_macro[0] , accs_macro[1]],index = analysis.columns)\n",
    "            analysis = analysis.append(to_append, ignore_index=True)\n",
    "            analysis.to_csv('classification_on_features.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = Force2Vec\n",
    "loss_name = 'Force2Vec'\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    if os.path.exists('../data_benchmark/graph_'+str(name)+'_attr.npy'):\n",
    "        print('hey')\n",
    "        for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "\n",
    "                MO = MainOptuna(name = name, conv = conv, device = device, loss_function = loss, mode = 'unsupervised')\n",
    "                best_values = MO.run(number_of_trials = 500)\n",
    "\n",
    "                loss_trgt = dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_force2vec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_Adj\n",
    "loss_name = 'VERSE_Adj'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    " \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_SR\n",
    "loss_name = 'VERSE_SR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = VERSE_PPR\n",
    "loss_name = 'VERSE_PPR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = LapEigen\n",
    "loss_name = 'LapEigen'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = LINE\n",
    "loss_name = 'LINE'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = GraphFactorization\n",
    "loss_name = 'GraphFactorization'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi, train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_CN\n",
    "loss_name = 'HOPE_CN'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_AA\n",
    "loss_name = 'HOPE_AA'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_RPR\n",
    "loss_name = 'HOPE_RPR'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = HOPE_Katz\n",
    "loss_name = 'HOPE_Katz'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"betta\"] = best_values['betta']\n",
    "                loss_trgt[\"lmbda\"] = best_values['lmbda']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = Node2Vec\n",
    "loss_name = 'Node2Vec'\n",
    "device = 'cpu'\n",
    "for name in ['chameleon']:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                loss_trgt[\"p\"] = best_values['p']\n",
    "                loss_trgt[\"q\"] = best_values['q']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = DeepWalk\n",
    "loss_name = 'DeepWalk'\n",
    "device='cpu'\n",
    "for name in ['Citeseer']:\n",
    "    for conv in ['GCN','GAT','SAGE']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"walks_per_node\"] = best_values['walk_length']\n",
    "                loss_trgt[\"walk_length\"] = best_values['walk_length']\n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"context_size\"] = best_values['context_size']\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = APP\n",
    "loss_name = 'APP'\n",
    "\n",
    "for name in datasets_names:\n",
    "    for conv in ['GCN']:\n",
    "            if len(analysis[ (analysis['loss'] == loss_name) & (analysis['conv'] == conv) & (analysis['dataset'] == name)] ) == 0:\n",
    "                \n",
    "                MO = MainOptuna(name=name,conv=conv, device=device, loss_function=loss,mode= 'unsupervised')\n",
    "                best_values=MO.run(number_of_trials=500)\n",
    "\n",
    "                loss_trgt=dict()\n",
    "                for par in loss:\n",
    "                    loss_trgt[par]=loss[par]\n",
    "   \n",
    "                \n",
    "                loss_trgt[\"num_negative_samples\"] = best_values['num_negative_samples']\n",
    "                loss_trgt[\"alpha\"] = best_values['alpha']\n",
    "\n",
    "                M = Main(name=name,conv=conv, device=device, loss_function=loss_trgt,mode= 'unsupervised')\n",
    "                train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma = M.run(best_values)\n",
    "\n",
    "                to_append=pd.Series([loss_name, conv,name, train_acc_mi, test_acc_mi,train_acc_ma , test_acc_ma],index = analysis.columns)\n",
    "                analysis = analysis.append(to_append,ignore_index=True)\n",
    "                analysis.to_csv('data_analysis_realdata.csv')\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}